ğŸ“„ README.md â€” Resume Parsing ETL Pipeline

ğŸš€ Project Overview

This project performs an end-to-end ETL (Extract â†’ Transform â†’ Load) pipeline for resume parsing using:

AWS S3 (input storage)

Python (ETL processing)

Regex-based text extraction (Name, Email, Skills, Education, Projects, Certifications, Experience)

MySQL Database (final storage)

Resumes can be uploaded in PDF or DOCX format.
The pipeline extracts text â†’ cleans â†’ structures â†’ loads into MySQL.

ğŸ—‚ Project Structure

      UNSTRUCTURED_DATA_2/
      â”‚
      â”œâ”€â”€ config/
      â”‚   â””â”€â”€ config.ini
      â”‚
      â”œâ”€â”€ src/
      â”‚   â”œâ”€â”€ extract.py
      â”‚   â”œâ”€â”€ transform.py
      â”‚   â”œâ”€â”€ load.py
      â”‚   â”œâ”€â”€ main.py
      â”‚   â”œâ”€â”€ config_reader.py
      â”‚   â”œâ”€â”€ temp_resume_text.txt     # Generated file (ignored by Git)
      â”‚   â””â”€â”€ dataframe.csv            # Generated output
      â”‚
      â”œâ”€â”€ .gitignore
      â””â”€â”€ README.md

âš™ï¸ Features

âœ” Extract

Download resume file from AWS S3

Read PDF using PyPDF2

Read DOCX using python-docx

Save text into temp_resume_text.txt

âœ” Transform

Your custom parser extracts:

Field	Description

  Name	First line detected in resume
  Email	Regex-based
  Summary	Objective / Summary section
  Skills	Detected from predefined skill keywords
  Experience_List	Extracted from INTERNSHIPS / numbered blocks
  Education	Academic details
  Certifications	Certificates / Licenses section
  Projects	Academic or personal projects
  Others	Everything not matching above sections

Output â†’ Clean DataFrame with 1 row per resume.

âœ” Load

MySQL table is created dynamically based on DataFrame columns

Python objects â†’ converted to SQL-friendly types

Lists â†’ stored as semicolon-separated strings

Inserts rows into MySQL safely

âš™ï¸ Configuration

ğŸ“ config/config.ini

[aws]
  bucket_name = s3-bucket-for-resume-parsing
  resume_key = incoming/resume1.pdf
  aws_access_key = YOUR_AWS_ACCESS_KEY
  aws_secret_key = YOUR_AWS_SECRET_KEY
  aws_region = ap-south-1
  
  [mysql]
  host = localhost
  user = root
  password =
  database = PythonLearningDB
  port = 3306


Never commit real credentials â€” .gitignore protects config.ini.

ğŸ›  How It Works

Step 1 â€” Extract

extract.py downloads and extracts text:

  raw_text = extract_and_save_text("temp_resume_text.txt")

Step 2 â€” Transform

transform.py parses the resume text:

  df = parse_resume_text(raw_text)

Step 3 â€” Load

load.py creates table and inserts data:

  conn = establish_connection()
  load_data(df, "parsed_resume")

â–¶ï¸ Running the Full Pipeline

From the root folder:

  cd src
  python main.py


This performs:

Download â†’

Text Extraction â†’

Parsing â†’

MySQL Insertion

ğŸ§ª Example Output (DataFrame)

  Name	Email	Skills	Summary	Experience_List	Education	Projects	Certifications	Others

Automatically generated from the resume text.

ğŸ“¦ Installation Requirements

Create a requirements.txt:

  boto3
  PyPDF2
  python-docx
  pandas
  mysql-connector-python


Install dependencies:

  pip install -r requirements.txt

ğŸ” Security Notes

config.ini contains sensitive information

âœ” Should never be committed

âœ” Protected via .gitignore

Use IAM User with restricted permissions for S3 access

Consider environment variables for production deployments

ğŸš§ Future Enhancements

Add phone number extraction

Improve section detection using AI/LLMs

Add support for .doc using textract / antiword

Build REST API using FastAPI

Deploy ETL pipeline on AWS Lambda

ğŸ“ Support

If you need help improving the parser or extending the ETL pipeline, feel free to reach out!
